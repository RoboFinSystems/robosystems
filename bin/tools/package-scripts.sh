#!/bin/bash
# Package and upload Lambda functions and UserData scripts to S3 for CloudFormation deployment

set -e

ENVIRONMENT="${1:-prod}"
BUCKET_NAME="robosystems-${ENVIRONMENT}-deployment"
REGION="${AWS_REGION:-us-east-1}"
REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"

echo "üì¶ Packaging Lambda functions and scripts for ${ENVIRONMENT} environment..."

# Create temporary directory
TEMP_DIR=$(mktemp -d)
trap "rm -rf $TEMP_DIR" EXIT

# Function to package a Lambda function
package_lambda() {
    local lambda_name=$1
    local source_file=$2
    local requirements=$3

    echo "üì¶ Packaging ${lambda_name} Lambda..."
    mkdir -p "$TEMP_DIR/${lambda_name}"

    # Copy the Lambda function
    cp "${REPO_ROOT}/bin/lambda/${source_file}" "$TEMP_DIR/${lambda_name}/index.py"

    # Create requirements.txt if needed
    if [ -n "$requirements" ]; then
        echo "$requirements" > "$TEMP_DIR/${lambda_name}/requirements.txt"

        # Install dependencies
        cd "$TEMP_DIR/${lambda_name}"
        pip install -r requirements.txt -t . --platform manylinux2014_x86_64 --only-binary=:all: --quiet
    fi

    # Create deployment package
    cd "$TEMP_DIR/${lambda_name}"
    zip -r "../${lambda_name}.zip" . -x "*.pyc" -x "__pycache__/*" > /dev/null

    # Calculate hash of the zip file
    local code_hash=$(openssl dgst -sha256 -binary "../${lambda_name}.zip" | openssl enc -base64)
    # Create a short hash for the filename (first 8 chars, alphanumeric only)
    local short_hash=$(echo "$code_hash" | tr -d '/+=' | cut -c1-8)
    echo "  Hash: ${code_hash} (short: ${short_hash})"

    # Upload to S3 with hash in filename
    echo "üì§ Uploading ${lambda_name} to S3..."
    local s3_key="lambda/${lambda_name}-${ENVIRONMENT}-${short_hash}.zip"
    aws s3 cp "../${lambda_name}.zip" "s3://${BUCKET_NAME}/${s3_key}" --region "$REGION"

    # Store the S3 key for CloudFormation to use
    echo "${s3_key}" > "../${lambda_name}.s3key"

    # Also store the full hash for reference
    echo "${code_hash}" > "../${lambda_name}.hash"
    aws s3 cp "../${lambda_name}.hash" "s3://${BUCKET_NAME}/lambda/${lambda_name}-${ENVIRONMENT}.hash" --region "$REGION"
}

# Package all Lambda functions

package_lambda "postgres-secrets" "postgres_secrets_update.py" "boto3==1.34.14"

package_lambda "postgres-rotation" "postgres_rotation.py" "boto3==1.34.14
psycopg2-binary==2.9.9"

package_lambda "valkey-rotation" "valkey_rotation.py" "boto3==1.34.14
redis==5.0.1"

package_lambda "graph-api-rotation" "graph_api_rotation.py" "boto3==1.34.14"

# Package snapshot Lambda functions (temporary - will be migrated to volume manager)
package_lambda "graph-snapshot-creator" "graph_snapshot_creator.py" "boto3==1.34.14"

package_lambda "graph-snapshot-selector" "graph_snapshot_selector.py" "boto3==1.34.14"

# Package volume management Lambda functions
package_lambda "graph-volume-manager" "graph_volume_manager.py" "boto3==1.34.14"

package_lambda "graph-volume-monitor" "graph_volume_monitor.py" "boto3==1.34.14
urllib3==2.0.7"

package_lambda "graph-volume-detachment" "graph_volume_detachment.py" "boto3==1.34.14"

# Package instance monitor Lambda function
package_lambda "graph-instance-monitor" "graph_instance_monitor.py" "boto3==1.34.14"

# Package worker monitor Lambda function (queue metrics and task protection)
package_lambda "worker-monitor" "worker_monitor.py" "boto3==1.34.14
redis==5.0.1"

# Upload UserData scripts
echo "üì§ Uploading UserData scripts to S3..."

# Upload all userdata scripts
for script in "${REPO_ROOT}"/bin/userdata/*.sh; do
    script_name=$(basename "$script")
    echo "  Uploading ${script_name}..."
    aws s3 cp "$script" "s3://${BUCKET_NAME}/userdata/${script_name}" --region "$REGION"
done

# Upload common userdata scripts (shared between Kuzu and Neo4j)
echo "üì§ Uploading common userdata scripts to S3..."
for script in "${REPO_ROOT}"/bin/userdata/common/*.sh; do
    if [ -f "$script" ]; then
        script_name=$(basename "$script")
        echo "  Uploading ${script_name}..."
        aws s3 cp "$script" "s3://${BUCKET_NAME}/userdata/common/${script_name}" --region "$REGION"
    fi
done

# Upload large CloudFormation templates to S3
echo "üì§ Uploading CloudFormation templates to S3..."

# Function to upload CloudFormation template if it's large
upload_cf_template() {
    local template_file=$1
    local template_name=$(basename "$template_file" .yaml)
    local file_size=$(stat -f%z "$template_file" 2>/dev/null || stat -c%s "$template_file" 2>/dev/null)
    local size_kb=$((file_size / 1024))

    # Upload if template is larger than 40KB
    # AWS CloudFormation inline template limit: 51,200 bytes (51KB)
    # Using 40KB threshold provides 11KB safety margin for:
    # - JSON formatting differences
    # - Parameter substitutions
    # - Metadata additions during deployment
    if [ $size_kb -gt 40 ]; then
        echo "  Uploading ${template_name}.yaml (${size_kb}KB) - exceeds inline limit..."
        aws s3 cp "$template_file" "s3://${BUCKET_NAME}/cloudformation/${template_name}-${ENVIRONMENT}.yaml" --region "$REGION"
    else
        echo "  Skipping ${template_name}.yaml (${size_kb}KB) - can be inlined"
    fi
}

# Upload CloudFormation templates that might exceed inline limits
for template in "${REPO_ROOT}"/cloudformation/*.yaml; do
    upload_cf_template "$template"
done

# Create a manifest file with all Lambda S3 keys and hashes for CloudFormation
echo "üìù Creating manifest for CloudFormation..."
MANIFEST="$TEMP_DIR/lambda-manifest-${ENVIRONMENT}.json"
echo "{" > "$MANIFEST"
echo "  \"Environment\": \"${ENVIRONMENT}\"," >> "$MANIFEST"
echo "  \"Timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"," >> "$MANIFEST"
echo "  \"Lambdas\": {" >> "$MANIFEST"

# Create manifest with S3 keys and hashes
FIRST=true
for lambda_name in postgres-secrets postgres-rotation valkey-rotation graph-api-rotation graph-snapshot-creator graph-snapshot-selector graph-volume-manager graph-volume-monitor graph-volume-detachment graph-instance-monitor worker-monitor; do
    if [ "$FIRST" = false ]; then
        echo "," >> "$MANIFEST"
    fi

    # Read the S3 key and hash from local files
    if [ -f "../${lambda_name}.s3key" ] && [ -f "../${lambda_name}.hash" ]; then
        S3_KEY=$(cat "../${lambda_name}.s3key")
        HASH=$(cat "../${lambda_name}.hash")
        echo -n "    \"${lambda_name}\": {\"s3_key\": \"${S3_KEY}\", \"hash\": \"${HASH}\"}" >> "$MANIFEST"
        FIRST=false
    fi
done

echo "" >> "$MANIFEST"
echo "  }" >> "$MANIFEST"
echo "}" >> "$MANIFEST"

# Upload the manifest
aws s3 cp "$MANIFEST" "s3://${BUCKET_NAME}/lambda/manifest-${ENVIRONMENT}.json" --region "$REGION"
echo "üì§ Lambda manifest uploaded successfully"

echo "‚úÖ Lambda functions, scripts, and CloudFormation templates packaged and uploaded successfully"
echo ""
echo "üìç S3 Artifacts:"
LAMBDA_COUNT=0
for lambda_name in postgres-secrets postgres-rotation valkey-rotation graph-api-rotation graph-snapshot-creator graph-snapshot-selector graph-volume-manager graph-volume-monitor graph-volume-detachment graph-instance-monitor worker-monitor; do
    if [ -f "../${lambda_name}.s3key" ]; then
        LAMBDA_COUNT=$((LAMBDA_COUNT + 1))
    fi
done
echo "  Lambda packages uploaded: $LAMBDA_COUNT functions"
echo "  Lambda manifest uploaded"
echo "  UserData scripts uploaded: 4 scripts (bastion, gha-runner, kuzu-writer, neo4j-writer)"
echo "  Shared userdata scripts uploaded: 6 scripts (graph-lifecycle, graph-health-check, register, run-container, setup-cloudwatch)"
echo ""
echo "üìã CloudFormation Parameters:"
echo "  LambdaCodeBucket configured"
echo "  Lambda Code Keys: Configured for $LAMBDA_COUNT functions"
echo "  UserData Script Keys: 4 main scripts + 6 shared scripts configured"
echo ""
echo "  CloudFormation Templates: Configured for ${ENVIRONMENT} environment"
